{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "new_word_discovery_with_information_entropy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPXs9LvCu35ENSs+ZxBNigU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhangxs131/New_word_discovery/blob/main/new_word_discovery_with_information_entropy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#    基于左右信息熵的新词发现算法\n",
        "\n",
        "源代码来自苏神的科学空间 https://spaces.ac.cn/archives/3491\n",
        "\n",
        "目前无聊的时候偶尔从前往后看看苏神博客，有意思的代码就跑一下\n",
        "\n",
        "数据集，苏神使用的是天龙八部，这里我打算使用斗罗大陆，个人比较喜欢但估计会比天龙八部跑的慢些。"
      ],
      "metadata": {
        "id": "XvuTCWJQeXTp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3uL84f-eKdc",
        "outputId": "0b8e4266-add1-44e8-ecde-8b74a2a49d64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "正在生成2字词...\n",
            "正在生成3字词...\n",
            "正在生成4字词...\n",
            "正在进行2字词的最大熵筛选(5360)...\n",
            "正在进行3字词的最大熵筛选(6469)...\n",
            "正在进行4字词的最大熵筛选(5898)...\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from numpy import log,min\n",
        "\n",
        "f = open('0009009.斗罗大陆.txt', 'r',encoding='gbk') #读取文章\n",
        "s = f.read() #读取为一个字符串\n",
        "\n",
        "#定义要去掉的标点字\n",
        "drop_dict = [u'，', u'\\n', u'。', u'、', u'：', u'(', u')', u'[', u']', u'.', u',', u' ', u'\\u3000', u'”', u'“', u'？', u'?', u'！', u'‘', u'’', u'…']\n",
        "for i in drop_dict: #去掉标点字\n",
        "    s = s.replace(i, '')\n",
        "\n",
        "#为了方便调用，自定义了一个正则表达式的词典\n",
        "myre = {2:'(..)', 3:'(...)', 4:'(....)', 5:'(.....)', 6:'(......)', 7:'(.......)'}\n",
        "\n",
        "min_count = 10 #录取词语最小出现次数\n",
        "min_support = 30 #录取词语最低支持度，1代表着随机组合\n",
        "min_s = 4 #录取词语最低信息熵，越大说明越有可能独立成词\n",
        "max_sep = 4 #候选词语的最大字数\n",
        "t=[] #保存结果用。\n",
        "\n",
        "t.append(pd.Series(list(s)).value_counts()) #逐字统计\n",
        "tsum = t[0].sum() #统计总字数\n",
        "rt = [] #保存结果用\n",
        "\n",
        "for m in range(2, max_sep+1):\n",
        "    print(u'正在生成%s字词...'%m)\n",
        "    t.append([])\n",
        "    for i in range(m): #生成所有可能的m字词\n",
        "        t[m-1] = t[m-1] + re.findall(myre[m], s[i:])\n",
        "    \n",
        "    t[m-1] = pd.Series(t[m-1]).value_counts() #逐词统计\n",
        "    t[m-1] = t[m-1][t[m-1] > min_count] #最小次数筛选\n",
        "    tt = t[m-1][:]\n",
        "    for k in range(m-1):\n",
        "        qq = np.array(list(map(lambda ms: tsum*t[m-1][ms]/t[m-2-k][ms[:m-1-k]]/t[k][ms[m-1-k:]], tt.index))) > min_support #最小支持度筛选。\n",
        "        tt = tt[qq]\n",
        "    rt.append(tt.index)\n",
        "\n",
        "def cal_S(sl): #信息熵计算函数\n",
        "    return -((sl/sl.sum()).apply(log)*sl/sl.sum()).sum()\n",
        "\n",
        "for i in range(2, max_sep+1):\n",
        "    print(u'正在进行%s字词的最大熵筛选(%s)...'%(i, len(rt[i-2])))\n",
        "    pp = [] #保存所有的左右邻结果\n",
        "    for j in range(i+2):\n",
        "        pp = pp + re.findall('(.)%s(.)'%myre[i], s[j:])\n",
        "    pp = pd.DataFrame(pp).set_index(1).sort_index() #先排序，这个很重要，可以加快检索速度\n",
        "    index = np.sort(np.intersect1d(rt[i-2], pp.index)) #作交集\n",
        "    #下面两句分别是左邻和右邻信息熵筛选\n",
        "    index = index[np.array(list(map(lambda s: cal_S(pd.Series(pp[0][s]).value_counts()), index))) > min_s]\n",
        "    rt[i-2] = index[np.array(list(map(lambda s: cal_S(pd.Series(pp[2][s]).value_counts()), index))) > min_s]\n",
        "\n",
        "#下面都是输出前处理\n",
        "for i in range(len(rt)):\n",
        "    t[i+1] = t[i+1][rt[i]]\n",
        "    t[i+1].sort_values(ascending = False)\n",
        "\n",
        "#保存结果并输出\n",
        "pd.DataFrame(pd.concat(t[1:])).to_csv('result.txt', header = False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('result.txt','r') as f:\n",
        "  content=f.readlines()\n",
        "\n",
        "words=[]\n",
        "score=[]\n",
        "for i in content:\n",
        "  words.append(i.split(',')[0])\n",
        "  score.append(int(i.split(',')[1]))\n",
        "\n",
        "df=pd.DataFrame({'words':words,'score':score})\n",
        "df=df.sort_values(by=\"score\",ascending=False)\n",
        "for i in df['words']:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfU8CpJEmERw",
        "outputId": "ef1ae707-c117-4474-efed-a46098d80f6b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "唐三\n",
            "自己\n",
            "已经\n",
            "没有\n",
            "他们\n",
            "小舞\n",
            "我们\n",
            "虽然\n",
            "现在\n",
            "此时\n",
            "大师\n",
            "能够\n",
            "同时\n",
            "戴沐白\n",
            "奥斯卡\n",
            "有些\n",
            "可以\n",
            "起来\n",
            "瞬间\n",
            "宁荣荣\n",
            "完全\n",
            "一般\n",
            "马红俊\n",
            "竟然\n",
            "开始\n",
            "武魂殿\n",
            "突然\n",
            "弗兰德\n",
            "修炼\n",
            "顿时\n",
            "并没有\n",
            "似乎\n",
            "比比东\n",
            "面前\n",
            "唐昊\n",
            "而且\n",
            "拥有\n",
            "对于\n",
            "千仞雪\n",
            "这种\n",
            "众人\n",
            "老师\n",
            "直接\n",
            "背后\n",
            "面对\n",
            "使用\n",
            "朱竹清\n",
            "如此\n",
            "泰坦\n",
            "胖子\n",
            "看上去\n",
            "立刻\n",
            "独孤博\n",
            "带着\n",
            "朝着\n",
            "骤然\n",
            "赵无极\n",
            "再次\n",
            "全部\n",
            "所以\n",
            "通过\n",
            "想要\n",
            "八蛛矛\n",
            "终于\n",
            "更加\n",
            "胡列娜\n",
            "宗门\n",
            "哪怕是\n",
            "原本\n",
            "进行\n",
            "极为\n",
            "波赛西\n",
            "作为\n",
            "渐渐\n",
            "除了\n",
            "对方\n",
            "宁风致\n",
            "昊天宗\n",
            "明显\n",
            "进入\n",
            "柳二龙\n",
            "仿佛\n",
            "先前\n",
            "今天\n",
            "在空中\n",
            "缓缓\n",
            "以及\n",
            "重新\n",
            "准备\n",
            "武魂帝国\n",
            "就像是\n",
            "大家\n",
            "白沉香\n",
            "曾经\n",
            "双手\n",
            "如同\n",
            "它们\n",
            "尤其是\n",
            "赶忙\n",
            "相比\n",
            "火舞\n",
            "继续\n",
            "十分\n",
            "宛如\n",
            "杨无敌\n",
            "凭借着\n",
            "再加上\n",
            "刚刚\n",
            "也无法\n",
            "悄然\n",
            "千雪\n",
            "风笑天\n",
            "一旦\n",
            "伴随着\n",
            "牛皋\n",
            "雪清河\n",
            "雪崩\n",
            "至于\n",
            "按照\n",
            "乃是\n",
            "泰隆\n",
            "白鹤\n",
            "雪夜大帝\n",
            "泰坦巨猿\n",
            "唐啸\n",
            "彻底\n",
            "并且\n",
            "可以说是\n",
            "海马斗罗\n",
            "楼高\n",
            "玉天恒\n",
            "吉祥\n",
            "暂时\n",
            "邪月\n",
            "即将\n",
            "七长老\n",
            "包括\n",
            "也不可能\n",
            "海龙斗罗\n",
            "由于\n",
            "甚至连\n",
            "各自\n",
            "泰诺\n",
            "秦明\n",
            "阿银\n",
            "金鳄斗罗\n",
            "孟依然\n",
            "迅速\n",
            "别说是\n",
            "王圣\n",
            "负责\n",
            "思龙\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "可以看出，大部分的人名都识别出来了，可以通过调整信息熵参数进行改变。"
      ],
      "metadata": {
        "id": "IQF2QE0OoPTm"
      }
    }
  ]
}